{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notes are based on 'Neural networks and deep learning' by Michael Nielsen, which can be found for free [here](http://neuralnetworksanddeeplearning.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What is a perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron is a type of neuron in a neural network. They were used a lot in the past, but today it is more common to use other models of artificial neurons.\n",
    "\n",
    "They take several binary inputs $x_1, x_2, \\ldots, x_n$ and produce a single binary output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets_deep_learning_research/perceptron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each input has an associated weight, $w$. The neuron's output is determined by whether the weighted sum is greather than some threshold value. \n",
    "\n",
    "Let $w_1, w_2, \\ldots, w_j$ be a vecotr of weights. Let $x_1, x_2, \\ldots, w_j$ be a vector of inputs to the neuron. Then the output of the neuron is described by the following formula:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "These vectors of weights and the threshold value are altered to create different models of decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What notation is more commonly used when describing how neurons work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted sum $\\sum_j w_j x_j$ is written as the dot product $w \\cdot x \\equiv \\sum_j w_j x_j$, where $w$ and $x$ are vectors whose components are the weights and inputs, respectively.\n",
    "\n",
    "The threshold is replaced by what is called the perceptron's *bias*, $b \\equiv -\\text{threshold}$. The bias is a measure of how easy it is for a perceptron to output $1$ (i.e. how easy it is to get a perceptron to *fire*). If the bias is large and positive, it is easy for the perceptron to output a $1$. If the bias is very negative, then it is difficult for the perceptron to output a $1$.\n",
    "\n",
    "Thus, the formula for the output of the perceptron is replaced with:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } w\\cdot x + b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } w\\cdot x + b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes learning using perceptrons difficult? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks learn by making changes to the weights and biases of the network. If a small change in weights or biases meant that it produced a small change in output, then we could figure out how to alter the weights and biases to get closer to correctly predicting some input. \n",
    "\n",
    "A small change in weights or bias of any single perceptron can sometimes cause the output to flip. This flip can cause the ouput of the entire network to change in unexpected ways. This makes it difficult to see how the network can model the desired behaviour by gradually modifying the weights and biases.\n",
    "\n",
    "An alternative is to use other models of neurons to aid us in our learning task. One such model of a neuron is the sigmoid neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a sigmoid neuron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sigmoid neuron is similar to the perceptron, but it is modified so that small changes in weights and biases cause only small changes in their outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics of the sigmoid neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of output being binary as in the perceptron's case, it can take on any value between $0$ and $1$.\n",
    "\n",
    "The sigmoid neuron also has weights for each input and an overall bias.\n",
    "\n",
    "The ouptut is no longer binary. It is defined by $\\sigma(w \\cdot x + b)$, where $\\sigma$ is the *sigmoid function*, which is defined by:\n",
    "\n",
    "$$\\sigma(z) \\equiv \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where $z \\equiv w \\cdot x + b$.\n",
    "\n",
    "In other words, given a vector of inputs $x_1, x_2, \\cdot, x_j$, a vector of weights $w_1, w_2, \\cdot, w_j$, and bias $b$, the output is defined as:\n",
    "\n",
    "$$\\frac{1}{1 + \\text{exp}(-(w \\cdot x + b))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the output of the sigmoid neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For very large $z$, the behaviour of the sigmoid neuron is similar to that of the perceptrons. \n",
    "\n",
    "If $z \\equiv w \\cdot x + b$ is a large positive number, then $e^{-z} \\approx 0$. Then:\n",
    "\n",
    "$$\\frac{1}{1 + e^{-z}} \\approx 1$$\n",
    "\n",
    "When $z$ is very negative, then $e^{-z} \\approx \\infty$ and:\n",
    "\n",
    "$$\\frac{1}{1 + e^{-z}} \\approx 0$$\n",
    "\n",
    "Where the behaviour of the sigmoid neuron differs from that of the behaviour of the perceptron is for those values in between these extremes.\n",
    "\n",
    "When plotted, the sigmoid function looks like this:\n",
    "\n",
    "<img src=\".\\assets_deep_learning_research\\sigmoid_function.png\">\n",
    "\n",
    "The output of the perceptron, when visualised as a step function, looks like this:\n",
    "\n",
    "<img src=\".\\assets_deep_learning_research\\step_function.png\">\n",
    "\n",
    "The smoothness of the sigmoid function, $\\sigma$, means that small changes $\\Delta w_j$ in the weights and $\\Delta b$ in the bias will produce a small change $\\Delta \\text{output}$ in the output from the neuron. $\\Delta \\text{output}$ is well approximated by:\n",
    "\n",
    "$$\\begin{eqnarray} \n",
    "  \\Delta \\mbox{output} \\approx \\sum_j \\frac{\\partial \\, \\mbox{output}}{\\partial w_j}\n",
    "  \\Delta w_j + \\frac{\\partial \\, \\mbox{output}}{\\partial b} \\Delta b,\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "where the sum is over all weights $w_j$ and $\\frac{\\partial \\text{output}}{\\partial w_j}$ and $\\frac{\\partial \\text{output}}{\\partial b}$ denote partial derivatives of the output with respect to $w_j$ and $b$, respectively. That is, $\\Delta \\text{output}$ is a *linear function* of changes $\\Delta w_j$ and $\\Delta b$ in the weights and bias. This linearity makes it easy to choose small changes in weights and biases to achieve a small change in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An activation function, $f(\\cdot)$ is the generalisation of the above, where the output of a neuron is $f(w \\cdot x + b)$ for some activation function.\n",
    "\n",
    "The main thing that changes when using different activation functions is that the particular values for the partial derivatives change in the above equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The architecture of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Leftmost layer are the *input neurons*.\n",
    "* Rightmost layers are the *output neurons*.\n",
    "* The middle layers are the *hidden layers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\assets_deep_learning_research\\network_architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why add more layers to a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deeper layers allow for the network to model increasingly abstract and complex decisions.\n",
    "\n",
    "In the below example, the first layer of the neural network is making simple decisions by weighing up the input evidence. The second layer is weighing up decisions made by the first layer, making more complex and abstract decisions than the first layer itself. The third layers is making even more complex and abstract decisions still."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\assets_deep_learning_research\\perceptron_layers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a feedforward network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network where the output from one layer is used as an input to the next layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a recurrent neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network where information can be fed backwards. They retain a state from one iteration to the next by using their own output as an input for the next step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
